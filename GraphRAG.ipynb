{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "heyP6m0-30Yp"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import wikipediaapi\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RNZN0wixvwP_"
      },
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "bm76ocm6v5xv"
      },
      "outputs": [],
      "source": [
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_community.vectorstores.neo4j_vector import Neo4jVector, remove_lucene_chars\n",
        "from langchain_google_genai import (GoogleGenerativeAI, GoogleGenerativeAIEmbeddings,\n",
        "                                    ChatGoogleGenerativeAI)\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import (RunnableParallel, RunnablePassthrough,\n",
        "                                      RunnableLambda)\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain.chains import (GraphCypherQAChain,\n",
        "                              RetrievalQAWithSourcesChain)\n",
        "from pydantic import BaseModel, Field, validator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SXKrdbWboomW"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWAe3gU--NSK"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XqRzFr7ZE6oz"
      },
      "outputs": [],
      "source": [
        "DOC_DIR = \"./documents\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "erSBD4vSgf7T"
      },
      "outputs": [],
      "source": [
        "# I run both commands as sometimes colab doesn't behave as expectedly\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GDSIq_3xUovO"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E7O5G-49K9xp"
      },
      "outputs": [],
      "source": [
        "# I need to create embeddings in multiple parts\n",
        "# for homogenity of the model, declaration is up\n",
        "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
        "EMBEDDINGS = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6vRUEUqFLPc"
      },
      "source": [
        "# Documents loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7GsNXJT_E2Ey"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(DOC_DIR):\n",
        "  os.mkdir(DOC_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59f1V8WNFEO1"
      },
      "source": [
        "**Please, either add your documents (.txt format) to the created folder, or run the scraping for my examples.**\n",
        "\n",
        "If adding own documents, the current default graph building might not work so well, you might need to customize \"Nodes\" and \"Relationships\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-7x35BNXXkUa"
      },
      "outputs": [],
      "source": [
        "def get_articles(keywords: list):\n",
        "  \"\"\"\n",
        "  Fetches and stores articles from wikipedia\n",
        "  Using keywords as title\n",
        "  \"\"\"\n",
        "  wiki_wiki = wikipediaapi.Wikipedia('GraphRAG', 'en')\n",
        "\n",
        "  for keyword in keywords:\n",
        "    try:\n",
        "      page_py = wiki_wiki.page(keyword)\n",
        "    except:\n",
        "      print(f\"Page with title {keyword} not found\")\n",
        "      continue\n",
        "    with open(os.path.join(DOC_DIR, f\"{keyword}.txt\"), \"w\") as f:\n",
        "      f.write(page_py.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GQWeYa5na5Vr"
      },
      "outputs": [],
      "source": [
        "keywords = [\"Retrieval-augmented_generation\", \"Large_language_model\", \"Natural_language_processing\",\n",
        "            \"LaMDA\", \"Groq\", \"Hugging_Face\",\n",
        "            \"ChatGPT\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xG4C1653Zm0Z"
      },
      "outputs": [],
      "source": [
        "get_articles(keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTWmtdRahuf7"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLEF4HMFGbE3"
      },
      "source": [
        "Optional function used in earlier development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ccxGwHGXdSII"
      },
      "outputs": [],
      "source": [
        "def get_paragraphs(dir_path: str, par_idx:int=0):\n",
        "  \"\"\"\n",
        "  Returns a dictionary of par_idx'th paragraph (value)\n",
        "                          for each filename (key) in dir_path\n",
        "  Assumes files are in .txt format\n",
        "  Assumes parragraphs are separated by a newline\n",
        "  \"\"\"\n",
        "\n",
        "  assert os.path.exists(dir_path), \"Directory does not exist\"\n",
        "  assert par_idx >= 0, \"par_idx must be non-negative\"\n",
        "\n",
        "  paragraphs = {}\n",
        "\n",
        "  for filename in os.listdir(dir_path):\n",
        "    if not filename.endswith(\".txt\"):\n",
        "      print(f\"{filename} is not a .txt, skippnig\")\n",
        "      continue\n",
        "    with open(os.path.join(dir_path, filename), \"r\") as f:\n",
        "      title = filename.split('.')[0]\n",
        "      text = f.read()\n",
        "\n",
        "      paragraphs_arr = text.split(\"\\n\")\n",
        "      if len(paragraphs_arr) <= par_idx:\n",
        "        print(f\"{title} document does not have {par_idx}th pargraph, skipping\")\n",
        "        continue\n",
        "\n",
        "      paragraphs[title] = paragraphs_arr[par_idx].strip()\n",
        "\n",
        "  return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_SsxzBGxPgjI"
      },
      "outputs": [],
      "source": [
        "def clean_text(text: str):\n",
        "  res = re.sub(r'\\s+', ' ', text)\n",
        "  res = re.sub('\\n\\n', '\\n', res)\n",
        "  return res\n",
        "\n",
        "\n",
        "def get_documents_with_metadata(dir_path: str):\n",
        "  \"\"\"\n",
        "  Returns an array of dictionries containing\n",
        "  text - document content\n",
        "  source - document title\n",
        "  id - document title + index\n",
        "  embedding - document embedding\n",
        "\n",
        "  Uses genai embedding model to create embeddings.\n",
        "  \"\"\"\n",
        "  assert os.path.exists(dir_path), \"Directory does not exist\"\n",
        "\n",
        "  documents_with_metadata = []\n",
        "  vectors = {}\n",
        "\n",
        "  for i,filename in enumerate(os.listdir(dir_path)):\n",
        "    if not filename.endswith(\".txt\"):\n",
        "      print(f\"{filename} is not a .txt, skipping\")\n",
        "      continue\n",
        "    with open(os.path.join(dir_path, filename), \"r\") as f:\n",
        "      text = f.read().strip()\n",
        "\n",
        "      # text = clean_text(text)\n",
        "\n",
        "      first_paragraph = text.split(\"\\n\")[0].strip()\n",
        "      title = filename.split('.')[0].strip()\n",
        "\n",
        "      documents_with_metadata.append({\n",
        "        'text': text,\n",
        "        'first_paragraph': first_paragraph,\n",
        "        'source': title,\n",
        "        'document_id': f'{title}-{i}',\n",
        "      })\n",
        "\n",
        "  return documents_with_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sYC6UyZX7a5V"
      },
      "outputs": [],
      "source": [
        "first_paragraphs = get_paragraphs(DOC_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe2i_-RPa6oL",
        "outputId": "b0b5aded-7232-4c29-a73e-f7bc2a423816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Natural_language_processing first paragraph has 512 characters\n",
            "ChatGPT first paragraph has 702 characters\n",
            "Groq first paragraph has 267 characters\n",
            "LaMDA first paragraph has 314 characters\n",
            "Retrieval-augmented_generation first paragraph has 446 characters\n",
            "Hugging_Face first paragraph has 411 characters\n",
            "Large_language_model first paragraph has 304 characters\n"
          ]
        }
      ],
      "source": [
        "for key, value in first_paragraphs.items():\n",
        "  print(f\"{key} first paragraph has {len(value)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLKc0GgNO7ah"
      },
      "source": [
        "All of the first paragraphs are small, I will use them without chunking for entity and relationship generation. Full text I will chunk up and store to index over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "leMycbP_Au1Z"
      },
      "outputs": [],
      "source": [
        "documents_with_metadata = get_documents_with_metadata(DOC_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ytvBC4UPW6"
      },
      "source": [
        "# NEO4J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgpcCERE5_zt"
      },
      "source": [
        "NEO4J is a widely used graph database management system. It provides multiple functinalities for graph management, querying and analysis. LangChain provides integraions with the syste.\n",
        "\n",
        "I used a free inctance of Aura DB for this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xWgPtGCXUQ2v"
      },
      "outputs": [],
      "source": [
        "# Neo4j provides a free instance on Aura which is more than enough for this HA\n",
        "# I don't see issue providing my credientials for this instance\n",
        "NEO4J_URL = \"neo4j+s://dd48e548.databases.neo4j.io\"\n",
        "NEO4J_USERNAME = \"neo4j\"\n",
        "NEO4J_PASSWORD = userdata.get('NEO_PASSWORD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0M8HwPkBVSMJ"
      },
      "outputs": [],
      "source": [
        "graph = Neo4jGraph(\n",
        "    url=NEO4J_URL, username=NEO4J_USERNAME, password=NEO4J_PASSWORD\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_vkCc6zp6fB3",
        "outputId": "f791d43e-170a-4978-ace4-122d9388dd79"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Graph is not empty, chances are it's already correctly                         populated. You can skip the following parts.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-ccfdc818bd4b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mRETURN\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnodeCount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \"\"\")\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnode_count\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0mGraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchances\u001b[0m \u001b[0mare\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0malready\u001b[0m \u001b[0mcorrectly\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                          \u001b[0mpopulated\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mYou\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mskip\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Graph is not empty, chances are it's already correctly                         populated. You can skip the following parts."
          ]
        }
      ],
      "source": [
        "node_count = graph.query(\"\"\"\n",
        "         MATCH (n)\n",
        "         RETURN count(n) as nodeCount\n",
        "         \"\"\")\n",
        "assert node_count==0, \"Graph is not empty, chances are it's already correctly\\\n",
        "                         populated. You can skip the following parts.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Atzt9jHv9q"
      },
      "source": [
        "If you want to proceed with the full pipeline, but the graph is already populated, run the following two cells to reset it to blank."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOTFWcyXH4SU",
        "outputId": "0a6ee2f0-ab9c-4b20-da7f-07b07c900675"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to remove all the nodes and their relationships\n",
        "graph.query(\"\"\"\n",
        "            MATCH (n)\n",
        "            DETACH DELETE n\n",
        "            \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOHzSYcBH8ny",
        "outputId": "1c6ceb7f-f7fa-4661-fd0a-b40698f7817a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# to check that there are no entried in the graph anymore\n",
        "graph.query(\"\"\"\n",
        "            CALL apoc.schema.assert({},{},true) YIELD label, key RETURN *\n",
        "            \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thAIM06j09by",
        "outputId": "3d5fa52c-ee76-434b-b6e0-a23f3cf2f177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# refreshing schema to view what's currently in the graph\n",
        "graph.refresh_schema()\n",
        "print(graph.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjPpMDZNXKkG"
      },
      "source": [
        "## Creating graph nodes using the first paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9LMOE7xGWG72"
      },
      "outputs": [],
      "source": [
        "def add_paragraphs_to_graph(documents_with_metadata: list):\n",
        "  \"\"\"\n",
        "  Adds pargraphs to the graph one by one\n",
        "  Return number of the processed documents (nodes created)\n",
        "  \"\"\"\n",
        "\n",
        "  # the query for creating a node\n",
        "  merge_paragraph_node_query = \"\"\"\n",
        "            MERGE(mergedParagraph:Paragraph {paragraphId: $paragraphParam.document_id})\n",
        "                ON CREATE SET\n",
        "                    mergedParagraph.source = $paragraphParam.source\n",
        "            RETURN mergedParagraph\n",
        "            \"\"\"\n",
        "\n",
        "  # ensure paragraph is in unique\n",
        "  graph.query(\"\"\"\n",
        "              CREATE CONSTRAINT unique_paragraph IF NOT EXISTS\n",
        "                  FOR (p:Paragraph) REQUIRE p.id IS UNIQUE\n",
        "              \"\"\")\n",
        "\n",
        "  # adding nodes one by one\n",
        "  node_count = 0\n",
        "  for paragraph in documents_with_metadata:\n",
        "    graph.query(merge_paragraph_node_query,\n",
        "            params={\n",
        "                'paragraphParam': paragraph\n",
        "            })\n",
        "    node_count += 1\n",
        "\n",
        "  print(f\"Created {node_count} nodes\")\n",
        "\n",
        "  return node_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYEDJXKGCGgV",
        "outputId": "6dfabd41-ed66-4c62-bdf3-c1530161d4f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 7 nodes\n"
          ]
        }
      ],
      "source": [
        "created_nodes = add_paragraphs_to_graph(documents_with_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "yZ7MPRoPXC5h"
      },
      "outputs": [],
      "source": [
        "node_count = graph.query(\"\"\"\n",
        "         MATCH (n)\n",
        "         RETURN count(n) as nodeCount\n",
        "         \"\"\")[0]['nodeCount']\n",
        "assert node_count==created_nodes, \"Number of created nodes does not equal\\\n",
        "                                  number of nodes in the graph\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5imF3sWqHirK",
        "outputId": "458d8588-0652-412f-ce75-bd7e9d3961d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "Paragraph {paragraphId: STRING, source: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# refresh schema to refer to the most recent one\n",
        "graph.refresh_schema()\n",
        "print(graph.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi3uTiROhy3u"
      },
      "source": [
        "## LLM for entity extraction, adding those entities to the graph.\n",
        "\n",
        "Note, that I limit the nodes and relationships using domain observations. I do that after graph inspection in Neo4j Aura.\n",
        "For other documents, this requires either customization or\n",
        "entity dismiguation in a more dedicated solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "NYxxMSnNGBTA"
      },
      "outputs": [],
      "source": [
        "def to_documents(documents_with_metadata: list):\n",
        "  \"\"\"\n",
        "  Converts documents_with_metadata to langchain documents\n",
        "  \"\"\"\n",
        "  return [Document(page_content=paragraph['first_paragraph'], metadata={'source': paragraph['source']}) for paragraph in documents_with_metadata]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "CY4C1RFlFUNf"
      },
      "outputs": [],
      "source": [
        "def transform_to_graph_documents(documents: list, allowed_nodes:list=[],\n",
        "                                allowed_relationships:list=[]):\n",
        "  \"\"\"\n",
        "  Transforms documents to graph documents using gemini-pro\n",
        "  \"\"\"\n",
        "\n",
        "  # I use Google models as they are free\n",
        "  # as opposed to the widely used OpenAI\n",
        "  llm = GoogleGenerativeAI(model=\"gemini-pro\")\n",
        "\n",
        "  llm_transformer = LLMGraphTransformer(\n",
        "    llm=llm,\n",
        "    allowed_nodes=allowed_nodes,\n",
        "    allowed_relationships=allowed_relationships,\n",
        "  )\n",
        "  graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
        "\n",
        "  # externally adding the source property for easier reference\n",
        "  # to the full document\n",
        "  for graph_document in graph_documents:\n",
        "    source = graph_document.source.metadata['source']\n",
        "    for node in graph_document.nodes:\n",
        "      node.properties = {\"source\": source}\n",
        "\n",
        "  return graph_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Bular1FCGQAN"
      },
      "outputs": [],
      "source": [
        "documents = to_documents(documents_with_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "H4A6B0LBE7Li"
      },
      "outputs": [],
      "source": [
        "allowed_nodes = [\"Technology\", \"Process\", \"Concept\", \"Field\"]\n",
        "allowed_relationships = [\"USED_FOR\", \"NOTABLE_FOR\", \"CAN\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hcJcKlEIu1hI",
        "outputId": "1f9600ff-d5d8-49e1-cc59-cd3f04abf11e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nodes:[Node(id='Natural language processing', type='Technology', properties={'source': 'Natural_language_processing'}), Node(id='text corpora', type='Concept', properties={'source': 'Natural_language_processing'}), Node(id='information retrieval', type='Process', properties={'source': 'Natural_language_processing'}), Node(id='computational linguistics', type='Process', properties={'source': 'Natural_language_processing'}), Node(id='process data encoded in natural language', type='Process', properties={'source': 'Natural_language_processing'}), Node(id='artificial intelligence', type='Field', properties={'source': 'Natural_language_processing'}), Node(id='machine learning', type='Process', properties={'source': 'Natural_language_processing'}), Node(id='knowledge representation', type='Process', properties={'source': 'Natural_language_processing'}), Node(id='computer science', type='Field', properties={'source': 'Natural_language_processing'}), Node(id='data', type='Concept', properties={'source': 'Natural_language_processing'}), Node(id='deep learning', type='Process', properties={'source': 'Natural_language_processing'}), Node(id='linguistics', type='Field', properties={'source': 'Natural_language_processing'})]\n",
            "Relationships:[Relationship(source=Node(id='Natural language processing', type='Technology', properties={}), target=Node(id='computer science', type='Field', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Natural language processing', type='Technology', properties={}), target=Node(id='artificial intelligence', type='Field', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Natural language processing', type='Technology', properties={}), target=Node(id='process data encoded in natural language', type='Process', properties={}), type='CAN', properties={}), Relationship(source=Node(id='Natural language processing', type='Technology', properties={}), target=Node(id='information retrieval', type='Process', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Natural language processing', type='Technology', properties={}), target=Node(id='knowledge representation', type='Process', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='Natural language processing', type='Technology', properties={}), target=Node(id='computational linguistics', type='Process', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='computational linguistics', type='Process', properties={}), target=Node(id='linguistics', type='Field', properties={}), type='NOTABLE_FOR', properties={}), Relationship(source=Node(id='data', type='Concept', properties={}), target=Node(id='text corpora', type='Concept', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='data', type='Concept', properties={}), target=Node(id='machine learning', type='Process', properties={}), type='USED_FOR', properties={}), Relationship(source=Node(id='data', type='Concept', properties={}), target=Node(id='deep learning', type='Process', properties={}), type='USED_FOR', properties={})]\n"
          ]
        }
      ],
      "source": [
        "graph_documents = transform_to_graph_documents(documents, allowed_nodes,\n",
        "                                               allowed_relationships)\n",
        "print(f\"Nodes:{graph_documents[0].nodes}\")\n",
        "print(f\"Relationships:{graph_documents[0].relationships}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gQrEPR8VwVgk"
      },
      "outputs": [],
      "source": [
        "graph.add_graph_documents(graph_documents,\n",
        "                          include_source=True,\n",
        "                          baseEntityLabel=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcAwDJphxF78",
        "outputId": "fa9692d3-43f5-4a4d-ec01-52cbade2e876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "Paragraph {paragraphId: STRING, source: STRING}\n",
            "Document {id: STRING, source: STRING, text: STRING}\n",
            "Concept {id: STRING, source: STRING}\n",
            "Process {id: STRING, source: STRING}\n",
            "Technology {id: STRING, source: STRING}\n",
            "Field {id: STRING, source: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Document)-[:MENTIONS]->(:Technology)\n",
            "(:Document)-[:MENTIONS]->(:Concept)\n",
            "(:Document)-[:MENTIONS]->(:Process)\n",
            "(:Document)-[:MENTIONS]->(:Field)\n",
            "(:Concept)-[:USED_FOR]->(:Concept)\n",
            "(:Concept)-[:USED_FOR]->(:Process)\n",
            "(:Process)-[:NOTABLE_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Process)\n",
            "(:Technology)-[:USED_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Technology)\n",
            "(:Technology)-[:USED_FOR]->(:Concept)\n",
            "(:Technology)-[:CAN]->(:Process)\n",
            "(:Technology)-[:CAN]->(:Concept)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Technology)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Process)\n"
          ]
        }
      ],
      "source": [
        "graph.refresh_schema()\n",
        "print(graph.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRtk9Q1VxSbo"
      },
      "source": [
        "## Creating embeddings for the entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "IWbHPqTmxZbs"
      },
      "outputs": [],
      "source": [
        "def update_with_embeddings(graph_documents: list):\n",
        "  \"\"\"\n",
        "  Updates the graph with embeddings for entities\n",
        "  using genai text embedding model\n",
        "  \"\"\"\n",
        "\n",
        "  graph.refresh_schema()\n",
        "\n",
        "  for graph_document in tqdm(graph_documents):\n",
        "    source = graph_document.source.metadata['source']\n",
        "    vectors = {}\n",
        "\n",
        "    for node in graph_document.nodes:\n",
        "      vectors[node.id] = genai.embed_content(\n",
        "            model=EMBEDDING_MODEL,\n",
        "            content=node.id,\n",
        "            task_type=\"retrieval_document\")['embedding']\n",
        "\n",
        "    graph.query(\"\"\"\n",
        "      MATCH (e:__Entity__) WHERE e.embedding IS NULL AND e.source = $source\n",
        "      SET e.embedding = $vectors[e.id]\n",
        "      \"\"\",\n",
        "          params={\"vectors\": vectors, \"source\": source})\n",
        "\n",
        "    graph.refresh_schema()\n",
        "  print(graph.schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hQ5YIBzHx1W",
        "outputId": "f79598a7-6415-406b-a6c9-7eccbfec5ef0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:16<00:00,  2.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "Paragraph {paragraphId: STRING, source: STRING}\n",
            "Document {id: STRING, source: STRING, text: STRING}\n",
            "Concept {id: STRING, source: STRING, embedding: LIST}\n",
            "Process {id: STRING, source: STRING, embedding: LIST}\n",
            "Technology {id: STRING, source: STRING, embedding: LIST}\n",
            "Field {id: STRING, source: STRING, embedding: LIST}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Document)-[:MENTIONS]->(:Technology)\n",
            "(:Document)-[:MENTIONS]->(:Concept)\n",
            "(:Document)-[:MENTIONS]->(:Process)\n",
            "(:Document)-[:MENTIONS]->(:Field)\n",
            "(:Concept)-[:USED_FOR]->(:Concept)\n",
            "(:Concept)-[:USED_FOR]->(:Process)\n",
            "(:Process)-[:NOTABLE_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Process)\n",
            "(:Technology)-[:USED_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Technology)\n",
            "(:Technology)-[:USED_FOR]->(:Concept)\n",
            "(:Technology)-[:CAN]->(:Process)\n",
            "(:Technology)-[:CAN]->(:Concept)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Technology)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Process)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "update_with_embeddings(graph_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d1Qyvq8K8o"
      },
      "source": [
        "## Creating explicit relationships between entities and source paragraphs.\n",
        "\n",
        "For some reason 1 document was dropped by add_graph_documents.\n",
        "\n",
        "This is a workaround of sorts to disambiguate nodes with no connections. Such is the case with Groq exracted entities, for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "AulU2aoX8Upc"
      },
      "outputs": [],
      "source": [
        "def add_pargraph_enetity_relationship():\n",
        "  \"\"\"\n",
        "  Adds explicit relationships between entities and source paragraphs\n",
        "  \"\"\"\n",
        "  graph.refresh_schema()\n",
        "\n",
        "  rel_query = \"\"\"\n",
        "              MATCH (e:__Entity__), (p:Paragraph) WHERE e.source = p.source\n",
        "              MERGE (e)<-[:HAS_ENTITY]-(p)\n",
        "              \"\"\"\n",
        "  graph.query(rel_query)\n",
        "\n",
        "  graph.refresh_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "qCNyfGM1L9cc"
      },
      "outputs": [],
      "source": [
        "def add_pagraph_document_relationship():\n",
        "  \"\"\"\n",
        "  Adds explicit relationships between paragraphs and documents\n",
        "  \"\"\"\n",
        "  graph.refresh_schema()\n",
        "\n",
        "  rel_query = \"\"\"\n",
        "            MATCH (p:Paragraph), (d:Document) WHERE p.source = d.source\n",
        "            MERGE (p)<-[:FIRST_CHUNK]-(d)\n",
        "            \"\"\"\n",
        "  graph.query(rel_query)\n",
        "\n",
        "  graph.refresh_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "wYJmFDkWMWDE"
      },
      "outputs": [],
      "source": [
        "add_pargraph_enetity_relationship()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYld4UNL_OJu",
        "outputId": "b272836a-d42a-4bd2-9caf-048673b4cf13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "Paragraph {paragraphId: STRING, source: STRING}\n",
            "Document {id: STRING, source: STRING, text: STRING}\n",
            "Concept {id: STRING, source: STRING, embedding: LIST}\n",
            "Process {id: STRING, source: STRING, embedding: LIST}\n",
            "Technology {id: STRING, source: STRING, embedding: LIST}\n",
            "Field {id: STRING, source: STRING, embedding: LIST}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Technology)\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Concept)\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Process)\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Field)\n",
            "(:Document)-[:MENTIONS]->(:Technology)\n",
            "(:Document)-[:MENTIONS]->(:Concept)\n",
            "(:Document)-[:MENTIONS]->(:Process)\n",
            "(:Document)-[:MENTIONS]->(:Field)\n",
            "(:Document)-[:FIRST_CHUNK]->(:Paragraph)\n",
            "(:Concept)-[:USED_FOR]->(:Concept)\n",
            "(:Concept)-[:USED_FOR]->(:Process)\n",
            "(:Process)-[:NOTABLE_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Process)\n",
            "(:Technology)-[:USED_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Technology)\n",
            "(:Technology)-[:USED_FOR]->(:Concept)\n",
            "(:Technology)-[:CAN]->(:Process)\n",
            "(:Technology)-[:CAN]->(:Concept)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Technology)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Process)\n"
          ]
        }
      ],
      "source": [
        "add_pagraph_document_relationship()\n",
        "print(graph.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZK-L39hJ5ZI"
      },
      "source": [
        "At this point I believe I have a working graph comparable with what is expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akP3qhzJ-sUd"
      },
      "source": [
        "# Storing the full documents for later use\n",
        "\n",
        "I will store them in Neo4j too, but mostly for convenience. They are not part of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "ow9t7k-Q-sB5"
      },
      "outputs": [],
      "source": [
        "def get_chunk_with_metadata(documents_with_metadata: List, chunk_size:int=512, chunk_overlap:int=24):\n",
        "  \"\"\"\n",
        "  Chunks up the full text and retains the metadata\n",
        "  \"\"\"\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = chunk_size,\n",
        "      chunk_overlap  = chunk_overlap,\n",
        "      length_function = len,\n",
        "      is_separator_regex = False,\n",
        "  )\n",
        "\n",
        "  chunks_with_metadata = []\n",
        "\n",
        "  for document_with_metadata in documents_with_metadata:\n",
        "    chunks = text_splitter.split_text(document_with_metadata['text'])\n",
        "    source = document_with_metadata['source']\n",
        "    for i,chunk in enumerate(chunks):\n",
        "      chunks_with_metadata.append({\n",
        "                                  'content': chunk,\n",
        "                                  'source': source,\n",
        "                                  'chunk_id': f'{source}-chunk-{i}'\n",
        "                                  })\n",
        "\n",
        "  return chunks_with_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "wMXIQePrFP34"
      },
      "outputs": [],
      "source": [
        "def add_chunks_to_neo(chunks: List):\n",
        "  \"\"\"\n",
        "  Stores chunks in Neo4j\n",
        "  \"\"\"\n",
        "\n",
        "  graph.refresh_schema()\n",
        "\n",
        "  # the query for creating a node\n",
        "  merge_chunk_node_query = \"\"\"\n",
        "            MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunk_id})\n",
        "                ON CREATE SET\n",
        "                    mergedChunk.source = $chunkParam.source,\n",
        "                    mergedChunk.content = $chunkParam.content\n",
        "            RETURN mergedChunk\n",
        "            \"\"\"\n",
        "\n",
        "  # ensure paragraph is in unique\n",
        "  graph.query(\"\"\"\n",
        "              CREATE CONSTRAINT unique_chunk IF NOT EXISTS\n",
        "                  FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
        "              \"\"\")\n",
        "\n",
        "  # adding nodes one by one\n",
        "  node_count = 0\n",
        "  for chunk in tqdm(chunks_with_metadata):\n",
        "    graph.query(merge_chunk_node_query,\n",
        "            params={\n",
        "                'chunkParam': chunk\n",
        "            })\n",
        "    node_count += 1\n",
        "\n",
        "  print(f\"Created {node_count} nodes\")\n",
        "\n",
        "  return node_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "KwzwgYECKi-o"
      },
      "outputs": [],
      "source": [
        "chunks_with_metadata = get_chunk_with_metadata(documents_with_metadata, 1000, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K28LSHxyK5p3",
        "outputId": "3baf4cdb-66f8-4e68-fc04-c50c188a8cc0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 239/239 [00:41<00:00,  5.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 239 nodes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "239"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "add_chunks_to_neo(chunks_with_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIHiMfNnGyb1",
        "outputId": "12601c34-afb1-4105-9b00-bec93ee18cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node properties:\n",
            "Paragraph {paragraphId: STRING, source: STRING}\n",
            "Document {id: STRING, source: STRING, text: STRING}\n",
            "Concept {id: STRING, source: STRING, embedding: LIST}\n",
            "Process {id: STRING, source: STRING, embedding: LIST}\n",
            "Technology {id: STRING, source: STRING, embedding: LIST}\n",
            "Field {id: STRING, source: STRING, embedding: LIST}\n",
            "Chunk {source: STRING, chunkId: STRING, content: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Technology)\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Concept)\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Process)\n",
            "(:Paragraph)-[:HAS_ENTITY]->(:Field)\n",
            "(:Document)-[:MENTIONS]->(:Technology)\n",
            "(:Document)-[:MENTIONS]->(:Concept)\n",
            "(:Document)-[:MENTIONS]->(:Process)\n",
            "(:Document)-[:MENTIONS]->(:Field)\n",
            "(:Document)-[:FIRST_CHUNK]->(:Paragraph)\n",
            "(:Concept)-[:USED_FOR]->(:Concept)\n",
            "(:Concept)-[:USED_FOR]->(:Process)\n",
            "(:Process)-[:NOTABLE_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Process)\n",
            "(:Technology)-[:USED_FOR]->(:Field)\n",
            "(:Technology)-[:USED_FOR]->(:Technology)\n",
            "(:Technology)-[:USED_FOR]->(:Concept)\n",
            "(:Technology)-[:CAN]->(:Process)\n",
            "(:Technology)-[:CAN]->(:Concept)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Technology)\n",
            "(:Technology)-[:NOTABLE_FOR]->(:Process)\n"
          ]
        }
      ],
      "source": [
        "graph.refresh_schema()\n",
        "print(graph.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkZPKEI8MH-_"
      },
      "source": [
        "# Initial search\n",
        "\n",
        "Plan:\n",
        "\n",
        "1. Use the discovered entities and relationships to determine which documents are relevant (based on the 'source' linking). The details on user query processing for this are below\n",
        "2. Collect the relevant chunks from the full documents into context\n",
        "3. Pass the model both (1) and (2) for it to have a view of existing concepts and relationships as well as a full context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVbBmU0gKed8"
      },
      "source": [
        "## Creating retrievers for structured (nodes, entities and their relationships).\n",
        "\n",
        "Langchain internal capabilties, while very helpful, failed to produce good Cypher queries for the chain.\n",
        "\n",
        "The selected texts are unstructured data consisting mainly of varied concepts (while on the same topic). For some data queries are predictable, so the prompt engineering is feasible while still intensive. For example, one might expect question like the following in a Hospital review dataset.\n",
        "  \n",
        "  Question: \"How many patients attended St. Mungus in September?\"\n",
        "\n",
        "  Query:\n",
        "  \n",
        "  MACTCH (p:Patient) WHERE p.hospital=St.Mungus AND p.month=Septermber\n",
        "  RETURN COUNT(p)\n",
        "\n",
        "Such query is successfully produced by llm for GraphCypherQAChain.\n",
        "\n",
        "However, I found that asking questions on concepts and ideas does not produce similar result. It seems that the chain as is insufficient for the chosen data.\n",
        "\n",
        "There was little to no leeway for the question spelling that would determine the success of the query. As such, questions like \"What are some nlp tasks?\" failed due to the fact that there wasn't necessarily an enitity labeled 'nlp', rather 'Natural Language Processing', while the llm failed to create a meaningful query than matching a label 1-1. After some attempts in prompting, I decided to employ a different strategy including embeddings.\n",
        "\n",
        "1. The model extracts appropriate entities from the user's query\n",
        "2. The graph is indexed over entities' short descriptions' embeddings\n",
        "3. The user's extracted entities are queried one by one over this index.\n",
        "4. For each node (within a limit), I output ANY relationship between the node and its neighbor\n",
        "5. The nodes' are used to filter chunks from which to query the full context, based on the shared source (original document).\n",
        "6. At the same time, the extracted relationships are passed as 'Structured data' to the chain for it to use the information and decide which type of relationships are most relevant.\n",
        "\n",
        "\n",
        "\n",
        "The following code is produced with the help of this blog:\n",
        "\n",
        "https://medium.com/neo4j/enhancing-the-accuracy-of-rag-applications-with-knowledge-graphs-ad5e2ffab663"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD3KXMvNW038",
        "outputId": "64051d9a-35ee-4fe5-f76b-b3a0ae231c36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.query(\n",
        "    \"\"\"CREATE FULLTEXT INDEX entity IF NOT EXISTS\n",
        "    FOR (e:__Entity__) ON EACH [e.id]\"\"\")\n",
        "\n",
        "graph.query(\n",
        "    \"\"\"CREATE VECTOR INDEX `entity_vector` IF NOT EXISTS\n",
        "    FOR (e:__Entity__) ON (e.embedding)\n",
        "    OPTIONS { indexConfig: {\n",
        "            `vector.dimensions`: 768,\n",
        "            `vector.similarity_function`: 'cosine'\n",
        "         }\n",
        "         }\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "WJ-yLAbJFi1L"
      },
      "outputs": [],
      "source": [
        "class Entities(BaseModel):\n",
        "    \"\"\"\n",
        "    Identifying information about entities.\n",
        "    \"\"\"\n",
        "\n",
        "    names: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"All the concept, field, process and technology entities that \"\n",
        "        \"appear in the text\",\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are extracting concept, field, process and technology entities from the text.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to extract information from the following \"\n",
        "            \"input: {question}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0,)\n",
        "entity_chain = prompt | llm.with_structured_output(Entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "UDVfGn6WG4Nl"
      },
      "outputs": [],
      "source": [
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text search.\n",
        "    It processes the input string by splitting it into words and appending a\n",
        "    similarity threshold (~2 changed characters) to each word, then combines\n",
        "    them using the AND operator. Useful for mapping entities from user questions\n",
        "    to database values, and allows for some misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()\n",
        "\n",
        "def generate_embedded_query(input: str) -> list:\n",
        "    \"\"\"\n",
        "    Generate an embedding for input\n",
        "    \"\"\"\n",
        "    return genai.embed_content(\n",
        "            model=EMBEDDING_MODEL,\n",
        "            content=input,\n",
        "            task_type=\"retrieval_query\")['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "NNA807LdRrrs"
      },
      "outputs": [],
      "source": [
        "def structured_retriever(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Collects the neighborhood of entities mentioned\n",
        "    in the question\n",
        "    \"\"\"\n",
        "    result = \"\"\n",
        "    sources = []\n",
        "    entities = entity_chain.invoke({\"question\": question})\n",
        "    for entity in entities.names:\n",
        "        response = graph.query(\n",
        "            \"\"\"CALL db.index.vector.queryNodes('entity_vector', 10, $query)\n",
        "              YIELD node, score\n",
        "              WITH node, score\n",
        "              CALL (){\n",
        "                MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
        "                RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
        "                UNION\n",
        "                MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
        "                RETURN neighbor.id + ' - ' + type(r) + ' -> ' +  node.id AS output\n",
        "              }\n",
        "              RETURN node.source AS source, output\n",
        "            \"\"\",\n",
        "            {\"query\": generate_embedded_query(entity)},\n",
        "        )\n",
        "        result += \"\\n\".join([el['output'] for el in response[:30] if el['output']])\n",
        "        sources.extend([el['source'] for el in response if el['source']])\n",
        "    return result, list(set(sources))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V5HL49zO1l4"
      },
      "source": [
        " ## Creating retrievers for unstructured (index over text embeddings) data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "9wDbldA-HVB-"
      },
      "outputs": [],
      "source": [
        "chunk_db = Neo4jVector.from_existing_graph(\n",
        "    EMBEDDINGS,\n",
        "    url=NEO4J_URL,\n",
        "    username=NEO4J_USERNAME,\n",
        "    password=NEO4J_PASSWORD,\n",
        "    index_name=\"document_index\",\n",
        "    node_label=\"Chunk\",\n",
        "    text_node_properties=[\"content\"],\n",
        "    embedding_node_property=\"embedding\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "DX1U_4_iUa5r"
      },
      "outputs": [],
      "source": [
        "def chunk_retriever(question: str, sources: List, k:int=3):\n",
        "  \"\"\"\n",
        "  Retrieves k most similar chunks filtered on the found relevant sources\n",
        "  \"\"\"\n",
        "  # Note that structured retriever can return an empty set\n",
        "  if len(sources) == 0:\n",
        "    documents = chunk_db.similarity_search(\n",
        "    question,\n",
        "    k=k\n",
        "  )\n",
        "  else:\n",
        "    documents = chunk_db.similarity_search(\n",
        "    question,\n",
        "    filter={\"source\": {\"$in\": sources}},\n",
        "    k=k\n",
        "  )\n",
        "  return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "MYXpMNDxVqMp"
      },
      "outputs": [],
      "source": [
        "def retriever(question: str):\n",
        "    # print(f\"Search query: {question}\")\n",
        "    structured_data, sources = structured_retriever(question)\n",
        "    unstructured_data = [el.page_content if isinstance(el.page_content, (str, bytes)) else str(el.page_content)\n",
        "        for el in chunk_retriever(question, sources, 10)]\n",
        "    # unstructured_data = [el if isinstance(el, (str, bytes)) else str(el)\n",
        "    #     for el in neo4j_vector_search(question, 'vector')]\n",
        "    final_data = f\"\"\"Structured data:\n",
        "                    {structured_data}\n",
        "                    Unstructured data:\n",
        "                    {\"#Document \". join(unstructured_data)}\"\"\"\n",
        "    # final_data = \"#Document \". join(unstructured_data)\n",
        "\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "1dvyLhL5Vv9y"
      },
      "outputs": [],
      "source": [
        "# template makes sure we answer only based on the provided files\n",
        "# and not hallucinate from the general llm capabilities\n",
        "template = \"\"\"Answer the question based only on the provided context.\n",
        "              Provide details.\n",
        "              If you are unsure, \"answer is not available in the context\",\n",
        "              don't provide the wrong answer.\n",
        "\n",
        "              Note, that context includes Structured and Unstrctured data:\n",
        "              Structured: contains relevant entities and their relationships\n",
        "              Unstructured: contains chunks of relevant text\n",
        "\n",
        "              Context: {context}\n",
        "\n",
        "              Question: {question}\n",
        "              \"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "# llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0.3,)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"context\": RunnableLambda(lambda x : x[\"question\"])|retriever,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zulD4ZazK8gq",
        "outputId": "ea968153-9175-42dc-882a-b2657be8c235"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Retrieval augmented generation (RAG) is a type of generative artificial intelligence that has information retrieval capabilities. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information in preference to information drawn from its own vast, static training data.'"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"question\": \"So what is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKipBDIW4Xu"
      },
      "source": [
        "# Pipeline evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4axhfNnu9UwA"
      },
      "source": [
        "I want to create a system that receives the same question as my GraphRAG, and the answer it gave, and evalutes wether the answer seems good.\n",
        "\n",
        "Addiitonally to that, I create a model that creates question.\n",
        "\n",
        "I then collect the scores for a simple glance of performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "bRefaNZkW4Co"
      },
      "outputs": [],
      "source": [
        "class Evaluation(BaseModel):\n",
        "    \"\"\"\n",
        "    Evaluating the quality of answer\n",
        "    \"\"\"\n",
        "\n",
        "    score: str = Field(\n",
        "        ...,\n",
        "        description=\"Whether the answer was consiered good (ok or not ok)\"\n",
        "    )\n",
        "    answer: str = Field(\n",
        "        ...,\n",
        "        description=\"The generated answer without changes\"\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are evaluating the quality of answer (ok or not ok) from the given answer to a question.\",\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Use the given format to evalate the following answer to the question \"\n",
        "            \"question: {question}, answer: {answer}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0.3,)\n",
        "eval_chain = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"answer\": RunnableLambda(lambda x : x) | chain,\n",
        "            \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "    | prompt\n",
        "    | llm.with_structured_output(Evaluation)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "ns06h07CE0rg"
      },
      "outputs": [],
      "source": [
        "class Questions(BaseModel):\n",
        "    \"\"\"\n",
        "    Evaluating the quality of answer\n",
        "    \"\"\"\n",
        "\n",
        "    questions: List[str] = Field(\n",
        "        ...,\n",
        "        description=\"A list of all the questions in the text\"\n",
        "    )\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Generate {number} relatively simple questions (one sentence, no commas)\"\n",
        "            \"on the following topics: {topics}\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "q_model = ChatGroq(model=\"mixtral-8x7b-32768\", temperature=0.3,)\n",
        "q_chain = prompt | q_model.with_structured_output(Questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "oIaRQYvpGq8s"
      },
      "outputs": [],
      "source": [
        "questions = q_chain.invoke({'number': \"30\", \"topics\": \"nlp, llm, chatgpt, groq, hugging face, lamda and rag\"}).questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQGZUqT8gfKa"
      },
      "source": [
        "This is a lengthy process, keep in mind."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5DzQgh-_t5z",
        "outputId": "fdd50a0b-8e51-4fcc-c9f0-8f579df762e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [14:16<00:00, 30.60s/it]\n"
          ]
        }
      ],
      "source": [
        "positive_answers = 0\n",
        "total_answers = 0\n",
        "for question in tqdm(questions):\n",
        "  score = eval_chain.invoke({\"question\": question}).score\n",
        "  positive_answers += score=='ok'\n",
        "  total_answers += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P24blkt8IiMv",
        "outputId": "066b2185-61cb-4ac8-e66f-2ec1951c5b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4642857142857143\n"
          ]
        }
      ],
      "source": [
        "print(positive_answers/total_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wieaD5mkNJdQ"
      },
      "source": [
        "Good and bad example of the produced result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZkUBxAqamEA",
        "outputId": "45bae736-4059-42f6-e775-9b6de34bd3ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['What is NLP?',\n",
              " 'What does LLM stand for?',\n",
              " 'What is ChatGPT?',\n",
              " 'What is Groq?',\n",
              " 'What is Hugging Face?',\n",
              " 'What is Lambda?',\n",
              " 'What is RAG?',\n",
              " 'How does NLP work?',\n",
              " 'What are the applications of LLM?',\n",
              " 'How is ChatGPT used?',\n",
              " 'What are the features of Groq?',\n",
              " 'What are the benefits of Hugging Face?',\n",
              " 'What are the use cases of Lambda?',\n",
              " 'What are the components of RAG?',\n",
              " 'What are the limitations of NLP?',\n",
              " 'What are the challenges of LLM?',\n",
              " 'What are the ethical considerations of ChatGPT?',\n",
              " 'How does Groq improve performance?',\n",
              " 'What are the unique features of Hugging Face?',\n",
              " 'What are the drawbacks of Lambda?',\n",
              " 'How does RAG enhance language understanding?',\n",
              " 'What are the future directions of NLP?',\n",
              " 'What are the research topics in LLM?',\n",
              " 'How is ChatGPT being developed?',\n",
              " 'What are the trends in Groq?',\n",
              " 'What are the applications of Hugging Face?',\n",
              " 'What are the innovations in Lambda?',\n",
              " 'What are the improvements in RAG?']"
            ]
          },
          "execution_count": 217,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUVk7EsSMqPN",
        "outputId": "45698f41-44f5-4477-c6ad-a8418e56c9fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Evaluation(score='ok', answer='The RAG process is made up of four key stages. First, all the data must be prepared and indexed for use by the LLM. Thereafter, each query consists of a retrieval, augmentation and a generation phase.')"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_chain.invoke({\"question\": 'What are the components of RAG?'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmdHO4KMM4HJ",
        "outputId": "f7cfa542-5a31-48c1-dd81-c9225eb2231f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Evaluation(score='not ok', answer='Answer is not available in the context')"
            ]
          },
          "execution_count": 219,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_chain.invoke({\"question\": 'What are the innovations in Lambda?'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDdGJ1oELq2Z"
      },
      "source": [
        "The ratio could be better.\n",
        "\n",
        "There may be several factors at play:\n",
        "\n",
        "Data:\n",
        "1. The documents I have selected may be not sharing the same topic from the perspective of llms and graph creation as much as it seemed to me.\n",
        "\n",
        "Design:\n",
        "1. Chunk size for the full text has not been experimented with extensively\n",
        "2. Number of k best chunks - similarly\n",
        "3. When searching for entities, there are 2 parameters that can be examined:  \n",
        "  * k - number of most similar nodes\n",
        "  * LIMIT <> - by how many connections to limit\n",
        "4. There are many missing connections in the graph between entities. I did not create \"SIMILAR\" connections nor did I create community reports like they do in the original GraphRAG. Providing such reports could possible improve the context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67Ca8AgrtMBx"
      },
      "source": [
        "# Unused code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtB8bwRY-x6e"
      },
      "outputs": [],
      "source": [
        "# Index the processed documents with FAISS for efficient retrieval\n",
        "db = FAISS.from_texts(\n",
        "    chunks,\n",
        "    EMBEDDINGS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S95nH_juBQmf"
      },
      "outputs": [],
      "source": [
        "def get_conversational_chain():\n",
        "    # Define a prompt template for asking questions based on a given context\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context, make sure to provide all the details,\n",
        "    if the answer is not in the provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
        "    Context:\\n {context}?\\n\n",
        "    Question: \\n{question}\\n\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a ChatGoogleGenerativeAI model for conversational AI\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "    # Create a prompt template with input variables \"context\" and \"question\"\n",
        "    prompt = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Load a question-answering chain with the specified model and prompt\n",
        "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psRfANp5CGWz"
      },
      "outputs": [],
      "source": [
        "question = \"Describe llm?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj5xpE5FBoC4",
        "outputId": "fd1eae36-5997-4d21-d941-0daab0f607e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'output_text': 'A large language model (LLM) is a computational model capable of language generation or other natural language processing tasks. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.'}\n"
          ]
        }
      ],
      "source": [
        "docs = db.similarity_search(question)\n",
        "\n",
        "# Obtain a conversational question-answering chain\n",
        "chain = get_conversational_chain()\n",
        "\n",
        "# Use the conversational chain to get a response based on the user question and retrieved documents\n",
        "response = chain(\n",
        "    {\"input_documents\": docs, \"question\": question}, return_only_outputs=True\n",
        ")\n",
        "\n",
        "# Print the response to the console\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1kMWAi9K25k"
      },
      "outputs": [],
      "source": [
        "def generate_full_text_query(input: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a full-text search query for a given input string.\n",
        "\n",
        "    This function constructs a query string suitable for a full-text search.\n",
        "    It processes the input string by splitting it into words and appending a\n",
        "    similarity threshold (~2 changed characters) to each word, then combines\n",
        "    them using the AND operator. Useful for mapping entities from user questions\n",
        "    to database values, and allows for some misspelings.\n",
        "    \"\"\"\n",
        "    full_text_query = \"\"\n",
        "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
        "    for word in words[:-1]:\n",
        "        full_text_query += f\" {word}~2 AND\"\n",
        "    full_text_query += f\" {words[-1]}~2\"\n",
        "    return full_text_query.strip()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
